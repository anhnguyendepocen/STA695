{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAYLOR-GANG AMES Housing Data\n",
    "\n",
    "## Theory and Methodology\n",
    "\n",
    "We stacked a LASSO and a Gradient Boosted Tree trained in XgBoost.\n",
    "\n",
    "### Stacking\n",
    "\n",
    "* Suppose we have M models $\\hat{F}(x) = (\\hat{f}_{1}(x), \\hat{f}_{2}, \\dots , \\hat{f}_{M}(x))$.\n",
    "* Under squared error loss, we can find the weight vector w such that $\\hat{w} =\\underset{w}{\\text{argmin}}\\hspace{.1in} {\\large E\\big[Y- w^{\\texttt{T}}\\hat{F} \\big]}^{2}$.\n",
    "* The solution $\\hat{w} = E(\\hat{F}^{\\texttt{T}}(x)\\hat{F}(x))^{-1} E(\\hat{F}(x) Y)$. \n",
    "* Under squared error loss, we can do no worse; i.e. ${\\large E(Y-\\hat{w}^{\\texttt{T}}\\hat{F}(x))^{2} \\leq E(Y- \\hat{f}_{m})}^{2}$ for $m = 1, \\dots, M$.\n",
    "\n",
    "<img src=\"2DJRgRvPQAODAit7NC1L_stacking.png\">\n",
    "\n",
    "Image source : https://www.commonlounge.com/discussion/1697ade39ac142988861daff4da7f27d\n",
    "\n",
    "**There can be even more layers added to this.**\n",
    "\n",
    "The above argument doesn't hold for other losses. But stacking is highly popular in data science competitions.\n",
    "\n",
    "For classification, we instead take a majority vote of the M classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Second Layer \n",
    "In the above, there is only one layer. But, we can add a second layer as well. This is often called **model blending**.\n",
    "\n",
    "\n",
    "<img src=\"workflow.png\">\n",
    "\n",
    "\n",
    "Image Source : https://www.kdnuggets.com/2017/02/stacking-models-imropved-predictions.html\n",
    "\n",
    "* So we start with our feature matrix X which is n $\\times$ p.\n",
    "* We then train M different models on X. \n",
    "* Each model then provides a prediction for the outcome vector y, which we call $\\widehat{y}^{m}$, for $m=1,\\dots, M$. \n",
    "* Now, define the augmentented $X^{(2)} = \\begin{pmatrix} \\widehat{y}^{1} & \\dots & \\widehat{y}^{M} \\end{pmatrix}$, which is an $n \\times M$ matrix. \n",
    "* Now, treat $X^{(2)}$ as the new feature matrix, and then can be trained to produce a final prediction vector, $\\widehat{y}^{final}$.\n",
    "\n",
    "#### Key Idea \n",
    "**Some models $\\widehat{f}$ predict better in certain regions of the feature space, even though they have lower overall accuracy. Blending (and its relatives) are a way combining several different models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO\n",
    "* Shrinkage method - Stepwise alogrithms can be highly variable in terms of choosing covariates. Moreover, prediction accuracy can be very poor. \n",
    "* Lasso (Least Absolute Shrinkage and Selection Operator) can achieve both of these goals. \n",
    "\n",
    "* **Idea** - Shrink unimportant regression coefficents towards zero, so we have variable selection. Moreover, Lasso is much less variable than stepwise regession, and so that we have smaller prediction error.\n",
    "\n",
    "We force regression cofficents towards zero by imposing that $\\|\\beta\\|$ must be lass than a fized value.\n",
    "\n",
    "#### Setup\n",
    "\n",
    "* Objective : minimize $\\|y - X\\beta\\|^{2}$ subject to $\\|\\beta\\| < t$, for a real number t.\n",
    "* So we have a balance of prediction accuracy, and model complexity (or regularization). \n",
    "* The $\\|\\beta\\|$ term shrinks non-important predictors towards zero, by a sort of soft threseholding. Read ESL 3.4.3 for more details about the shrinking.\n",
    "* Typically, write the objective function in a Lagrange multiplier form as \n",
    "\n",
    "$\\widehat{\\beta}^{Lasso} = \\underset{\\beta}{argmin}\\lbrace \\|y - X\\beta\\|^{2} + \\lambda \\|\\beta\\| \\rbrace $\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
