
@article{pelt_mixed-scale_2018,
	title = {A mixed-scale dense convolutional neural network for image analysis},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1715832114},
	doi = {10/gct9cx},
	language = {en},
	number = {2},
	urldate = {2018-04-24},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Pelt, Daniël M. and Sethian, James A.},
	month = jan,
	year = {2018},
	keywords = {ToRead},
	pages = {254--259},
	file = {Pelt and Sethian - 2018 - A mixed-scale dense convolutional neural network f.pdf:D\:\\download\\Zotero\\storage\\NLNZ4263\\Pelt and Sethian - 2018 - A mixed-scale dense convolutional neural network f.pdf:application/pdf}
}

@inproceedings{defferrard_convolutional_2016,
	title = {Convolutional {Neural} {Networks} on {Graphs} with {Fast} {Localized} {Spectral} {Filtering}},
	abstract = {In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words’ embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efﬁcient numerical schemes to design fast localized convolutional ﬁlters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
	language = {en},
	booktitle = {{NIPS}},
	author = {Defferrard, Michaël and Bresson, Xavier and Vandergheynst, Pierre},
	year = {2016},
	keywords = {ToRead},
	pages = {9},
	file = {Defferrard et al. - Convolutional Neural Networks on Graphs with Fast .pdf:D\:\\download\\Zotero\\storage\\96IN8LI3\\Defferrard et al. - Convolutional Neural Networks on Graphs with Fast .pdf:application/pdf}
}

@article{li_adaptive_nodate,
	title = {Adaptive {Graph} {Convolutional} {Neural} {Networks}},
	abstract = {Graph Convolutional Neural Networks (Graph CNNs) are generalizations of classical CNNs to handle graph data such as molecular data, point could and social networks. Current ﬁlters in graph CNNs are built for ﬁxed and shared graph structure. However, for most real data, the graph structures varies in both size and connectivity. The paper proposes a generalized and ﬂexible graph CNN taking data of arbitrary graph structure as input. In that way a task-driven adaptive graph is learned for each graph data while training. To efﬁciently learn the graph, a distance metric learning is proposed. Extensive experiments on nine graph-structured datasets have demonstrated the superior performance improvement on both convergence speed and predictive accuracy.},
	language = {en},
	author = {Li, Ruoyu and Wang, Sheng and Zhu, Feiyun and Huang, Junzhou},
	keywords = {Graph Deep Learning, ToRead},
	pages = {8},
	file = {Li et al. - Adaptive Graph Convolutional Neural Networks.pdf:D\:\\download\\Zotero\\storage\\AY6I7S5T\\Li et al. - Adaptive Graph Convolutional Neural Networks.pdf:application/pdf}
}

@article{burt_laplacian_nodate,
	title = {The {Laplacian} {Pyramid} as a {Compact} {Image} {Code}},
	language = {en},
	author = {BURT, PETER J},
	pages = {9},
	file = {BURT - The Laplacian Pyramid as a Compact Image Code.pdf:D\:\\download\\Zotero\\storage\\S6PMG5I2\\BURT - The Laplacian Pyramid as a Compact Image Code.pdf:application/pdf}
}

@article{scholkopf_nonlinear_1998,
	title = {Nonlinear {Component} {Analysis} as a {Kernel} {Eigenvalue} {Problem}},
	volume = {10},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/10.1162/089976698300017467},
	doi = {10.1162/089976698300017467},
	language = {en},
	number = {5},
	urldate = {2018-04-24},
	journal = {Neural Computation},
	author = {Schölkopf, Bernhard and Smola, Alexander and Müller, Klaus-Robert},
	month = jul,
	year = {1998},
	pages = {1299--1319},
	file = {Schölkopf et al. - 1998 - Nonlinear Component Analysis as a Kernel Eigenvalu.pdf:D\:\\download\\Zotero\\storage\\A7ML4HVS\\Schölkopf et al. - 1998 - Nonlinear Component Analysis as a Kernel Eigenvalu.pdf:application/pdf}
}

@article{rasti_macular_2018,
	title = {Macular {OCT} {Classification} {Using} a {Multi}-{Scale} {Convolutional} {Neural} {Network} {Ensemble}},
	volume = {37},
	issn = {0278-0062, 1558-254X},
	url = {http://ieeexplore.ieee.org/document/8166817/},
	doi = {10/gdcdrc},
	abstract = {Computer-aided diagnosis (CAD) of retinal pathologies is a current active area in medical image analysis. Due to the increasing use of retinal optical coherence tomography (OCT) imaging technique, a CAD system in retinal OCT is essential to assist ophthalmologist in the early detection of ocular diseases and treatment monitoring. This paper presents a novel CAD system based on a multi-scale convolutional mixture of expert (MCME) ensemble model to identify normal retina, and two common types of macular pathologies, namely, dry age-related macular degeneration, and diabetic macular edema. The proposed MCME modular model is a data-driven neural structure, which employs a new cost function for discriminative and fast learning of image features by applying convolutional neural networks on multiple-scale sub-images. MCME maximizes the likelihood function of the training data set and ground truth by considering a mixture model, which tries also to model the joint interaction between individual experts by using a correlated multivariate component for each expert module instead of only modeling the marginal distributions by independent Gaussian components. Two different macular OCT data sets from Heidelberg devices were considered for the evaluation of the method, i.e., a local data set of OCT images of 148 subjects and a public data set of 45 OCT acquisitions. For comparison purpose, we performed a wide range of classiﬁcation measures to compare the results with the best conﬁgurations of the MCME method. With the MCME model of four scale-dependent experts, the precision rate of 98.86\%, and the area under the receiver operating characteristic curve (AUC) of 0.9985 were obtained on average.},
	language = {en},
	number = {4},
	urldate = {2018-04-24},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Rasti, Reza and Rabbani, Hossein and Mehridehnavi, Alireza and Hajizadeh, Fedra},
	month = apr,
	year = {2018},
	pages = {1024--1034},
	file = {Rasti et al. - 2018 - Macular OCT Classification Using a Multi-Scale Con.pdf:D\:\\download\\Zotero\\storage\\H56XSTN5\\Rasti et al. - 2018 - Macular OCT Classification Using a Multi-Scale Con.pdf:application/pdf}
}

@article{riazi_graph_nodate,
	title = {Graph {Representation} {Learning} and {Graph} {Classification}},
	language = {en},
	author = {Riazi, Sara},
	keywords = {ToRead},
	pages = {36},
	file = {Riazi - Graph Representation Learning and Graph Classifica.pdf:D\:\\download\\Zotero\\storage\\A9G2TQBF\\Riazi - Graph Representation Learning and Graph Classifica.pdf:application/pdf}
}

@article{shelhamer_fully_2017,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	volume = {39},
	issn = {0162-8828, 2160-9292},
	url = {http://ieeexplore.ieee.org/document/7478072/},
	doi = {10/f9xn4v},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efﬁcient inference and learning. We deﬁne and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classiﬁcation networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by ﬁne-tuning to the segmentation task. We then deﬁne a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, ﬁne layer to produce accurate and detailed segmentations. Our fully convolutional networks achieve improved segmentation of PASCAL VOC (30\% relative improvement to 67.2\% mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.},
	language = {en},
	number = {4},
	urldate = {2018-04-24},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
	month = apr,
	year = {2017},
	keywords = {ToRead},
	pages = {640--651},
	file = {Shelhamer et al. - 2017 - Fully Convolutional Networks for Semantic Segmenta.pdf:D\:\\download\\Zotero\\storage\\ZHFVF3UK\\Shelhamer et al. - 2017 - Fully Convolutional Networks for Semantic Segmenta.pdf:application/pdf}
}

@article{sze_efficient_2017,
	title = {Efficient {Processing} of {Deep} {Neural} {Networks}: {A} {Tutorial} and {Survey}},
	volume = {105},
	issn = {0018-9219, 1558-2256},
	shorttitle = {Efficient {Processing} of {Deep} {Neural} {Networks}},
	url = {http://ieeexplore.ieee.org/document/8114708/},
	doi = {10/gcnp38},
	language = {en},
	number = {12},
	urldate = {2018-04-24},
	journal = {Proceedings of the IEEE},
	author = {Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S.},
	month = dec,
	year = {2017},
	keywords = {Survey, ToRead},
	pages = {2295--2329},
	file = {Sze et al. - 2017 - Efficient Processing of Deep Neural Networks A Tu.pdf:D\:\\download\\Zotero\\storage\\4Z25MC8Q\\Sze et al. - 2017 - Efficient Processing of Deep Neural Networks A Tu.pdf:application/pdf}
}

@article{vogt_discriminant_nodate,
	title = {Discriminant {NAP} for {SVM} {Speaker} {Recognition}},
	abstract = {Nuisance Attribute Projection (NAP) provides an effective method of removing the unwanted session variability in a Support Vector Machine (SVM) based speaker recognition system by removing the principal components of this variability. There is no guarantee with the methods proposed, however, that desired speaker variability is retained.},
	language = {en},
	author = {Vogt, Robbie and Kajarekar, Sachin and Sridharan, Sridha},
	pages = {6},
	file = {Vogt et al. - Discriminant NAP for SVM Speaker Recognition.pdf:D\:\\download\\Zotero\\storage\\ZBVZWXY5\\Vogt et al. - Discriminant NAP for SVM Speaker Recognition.pdf:application/pdf}
}

@inproceedings{huang_densely_2017,
	title = {Densely {Connected} {Convolutional} {Networks}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099726/},
	doi = {10.1109/CVPR.2017.243},
	language = {en},
	urldate = {2018-04-24},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Huang, Gao and Liu, Zhuang and Maaten, Laurens van der and Weinberger, Kilian Q.},
	month = jul,
	year = {2017},
	keywords = {ToRead},
	pages = {2261--2269},
	file = {Huang et al. - 2017 - Densely Connected Convolutional Networks.pdf:D\:\\download\\Zotero\\storage\\RVQSHBJV\\Huang et al. - 2017 - Densely Connected Convolutional Networks.pdf:application/pdf}
}

@article{akkus_deep_2017,
	title = {Deep {Learning} for {Brain} {MRI} {Segmentation}: {State} of the {Art} and {Future} {Directions}},
	volume = {30},
	issn = {0897-1889, 1618-727X},
	shorttitle = {Deep {Learning} for {Brain} {MRI} {Segmentation}},
	url = {http://link.springer.com/10.1007/s10278-017-9983-4},
	doi = {10.1007/s10278-017-9983-4},
	abstract = {Quantitative analysis of brain MRI is routine for many neurological diseases and conditions and relies on accurate segmentation of structures of interest. Deep learning-based segmentation approaches for brain MRI are gaining interest due to their self-learning and generalization ability over large amounts of data. As the deep learning architectures are becoming more mature, they gradually outperform previous state-of-the-art classical machine learning algorithms. This review aims to provide an overview of current deep learning-based segmentation approaches for quantitative brain MRI. First we review the current deep learning architectures used for segmentation of anatomical brain structures and brain lesions. Next, the performance, speed, and properties of deep learning approaches are summarized and discussed. Finally, we provide a critical assessment of the current state and identify likely future developments and trends.},
	language = {en},
	number = {4},
	urldate = {2018-04-24},
	journal = {Journal of Digital Imaging},
	author = {Akkus, Zeynettin and Galimzianova, Alfiia and Hoogi, Assaf and Rubin, Daniel L. and Erickson, Bradley J.},
	month = aug,
	year = {2017},
	keywords = {Survey, ToRead},
	pages = {449--459},
	file = {Akkus et al. - 2017 - Deep Learning for Brain MRI Segmentation State of.pdf:D\:\\download\\Zotero\\storage\\E3PZQNKR\\Akkus et al. - 2017 - Deep Learning for Brain MRI Segmentation State of.pdf:application/pdf}
}

@article{lu_cot:_2018,
	title = {{CoT}: {Cooperative} {Training} for {Generative} {Modeling}},
	shorttitle = {{CoT}},
	url = {http://arxiv.org/abs/1804.03782},
	abstract = {We propose Cooperative Training (CoT) for training generative models that measure a tractable density function for target data. CoT coordinately trains a generator \$G\$ and an auxiliary predictive mediator \$M\$. The training target of \$M\$ is to estimate a mixture density of the learned distribution \$G\$ and the target distribution \$P\$, and that of \$G\$ is to minimize the Jensen-Shannon divergence estimated through \$M\$. CoT achieves independent success without the necessity of pre-training via Maximum Likelihood Estimation or involving high-variance algorithms like REINFORCE. This low-variance algorithm is theoretically proved to be unbiased for both generative and predictive tasks. We also theoretically and empirically show the superiority of CoT over most previous algorithms, in terms of generative quality and diversity, predictive generalization ability and computational cost.},
	urldate = {2018-04-24},
	journal = {arXiv:1804.03782 [cs, stat]},
	author = {Lu, Sidi and Yu, Lantao and Zhang, Weinan and Yu, Yong},
	month = apr,
	year = {2018},
	keywords = {ToRead},
	file = {arXiv\:1804.03782 PDF:D\:\\download\\Zotero\\storage\\L8A9IE85\\Lu et al. - 2018 - CoT Cooperative Training for Generative Modeling.pdf:application/pdf;arXiv.org Snapshot:D\:\\download\\Zotero\\storage\\RUB3JKYK\\1804.html:text/html}
}

@inproceedings{velickovic_graph_2018,
	title = {{GRAPH} {ATTENTION} {NETWORKS}},
	abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods’ features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of computationally intensive matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
	language = {en},
	booktitle = {{ICLR}},
	author = {Velicˇkovic, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana},
	year = {2018},
	keywords = {Graph Deep Learning},
	file = {Velicˇkovic et al. - 2018 - GRAPH ATTENTION NETWORKS.pdf:D\:\\download\\Zotero\\storage\\5JQZAUMP\\Velicˇkovic et al. - 2018 - GRAPH ATTENTION NETWORKS.pdf:application/pdf}
}

@inproceedings{gong_shape_2009,
	title = {Shape of {Gaussians} as feature descriptors},
	doi = {10.1109/CVPR.2009.5206506},
	abstract = {This paper introduces a feature descriptor called shape of Gaussian (SOG), which is based on a general feature descriptor design framework called shape of signal probability density function (SOSPDF). SOSPDF takes the shape of a signal's probability density function (pdf) as its feature. Under such a view, both histogram and region covariance often used in computer vision are SOSPDF features. Histogram describes SOSPDF by a discrete approximation way. Region covariance describes SOSPDF as an incomplete parameterized multivariate Gaussian distribution. Our proposed SOG descriptor is a full parameterized Gaussian, so it has all the advantages of region covariance and is more effective. Furthermore, we identify that SOGs form a Lie group. Based on Lie group theory, we propose a distance metric for SOG. We test SOG features in tracking problem. Experiments show better tracking results compared with region covariance. Moreover, experiment results indicate that SOG features attempt to harvest more useful information and are less sensitive against noise.},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Gong, Liyu and Wang, Tianjiang and Liu, Fang},
	month = jun,
	year = {2009},
	pages = {2366--2371},
	file = {IEEE Xplore Abstract Record:D\:\\download\\Zotero\\storage\\XDAKQGU9\\5206506.html:text/html;IEEE Xplore Full Text PDF:D\:\\download\\Zotero\\storage\\SCY72Y9N\\Gong et al. - 2009 - Shape of Gaussians as feature descriptors.pdf:application/pdf}
}

@article{alom_history_2018,
	title = {The {History} {Began} from {AlexNet}: {A} {Comprehensive} {Survey} on {Deep} {Learning} {Approaches}},
	shorttitle = {The {History} {Began} from {AlexNet}},
	url = {http://arxiv.org/abs/1803.01164},
	abstract = {Deep learning has demonstrated tremendous success in variety of application domains in the past few years. This new field of machine learning has been growing rapidly and applied in most of the application domains with some new modalities of applications, which helps to open new opportunity. There are different methods have been proposed on different category of learning approaches, which includes supervised, semi-supervised and un-supervised learning. The experimental results show state-of-the-art performance of deep learning over traditional machine learning approaches in the field of Image Processing, Computer Vision, Speech Recognition, Machine Translation, Art, Medical imaging, Medical information processing, Robotics and control, Bio-informatics, Natural Language Processing (NLP), Cyber security, and many more. This report presents a brief survey on development of DL approaches, including Deep Neural Network (DNN), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) including Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). In addition, we have included recent development of proposed advanced variant DL techniques based on the mentioned DL approaches. Furthermore, DL approaches have explored and evaluated in different application domains are also included in this survey. We have also comprised recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys have published on Deep Learning in Neural Networks [1, 38] and a survey on RL [234]. However, those papers have not discussed the individual advanced techniques for training large scale deep learning models and the recently developed method of generative models [1].},
	urldate = {2018-04-24},
	journal = {arXiv:1803.01164 [cs]},
	author = {Alom, Md Zahangir and Taha, Tarek M. and Yakopcic, Christopher and Westberg, Stefan and Hasan, Mahmudul and Van Esesn, Brian C. and Awwal, Abdul A. S. and Asari, Vijayan K.},
	month = mar,
	year = {2018},
	keywords = {Survey, ToRead},
	file = {Alom et al_2018_The History Began from AlexNet.pdf:D\:\\download\\Zotero\\storage\\7BIS4EDI\\Alom et al_2018_The History Began from AlexNet.pdf:application/pdf;arXiv.org Snapshot:D\:\\download\\Zotero\\storage\\V2TNU5ER\\1803.html:text/html}
}

@article{sen_collective_2008,
	title = {Collective {Classification} in {Network} {Data}},
	volume = {29},
	copyright = {Copyright Association for the Advancement of Artificial Intelligence Fall 2008},
	issn = {07384602},
	url = {http://search.proquest.com/docview/208134405/abstract/7CDC6CCFDEB4610PQ/1},
	language = {English},
	number = {3},
	urldate = {2018-04-25},
	journal = {AI Magazine; La Canada},
	author = {Sen, Prithviraj and Namata, Galileo and Bilgic, Mustafa and Getoor, Lise and Gallagher, Brian and Eliassi-Rad, Tina},
	year = {2008},
	pages = {93--106},
	file = {Sen et al_2008_Collective Classification in Network Data.pdf:D\:\\download\\Zotero\\storage\\CB3BIL9F\\Sen et al_2008_Collective Classification in Network Data.pdf:application/pdf}
}

@article{hamilton_representation_2017,
	title = {Representation {Learning} on {Graphs}: {Methods} and {Applications}},
	shorttitle = {Representation {Learning} on {Graphs}},
	url = {http://arxiv.org/abs/1709.05584},
	abstract = {Machine learning on graphs is an important and ubiquitous task with applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e.g., degree statistics or kernel functions). However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks. We review methods to embed individual nodes as well as approaches to embed entire (sub)graphs. In doing so, we develop a unified framework to describe these recent approaches, and we highlight a number of important applications and directions for future work.},
	urldate = {2018-04-25},
	journal = {arXiv:1709.05584 [cs]},
	author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
	month = sep,
	year = {2017},
	keywords = {Graph Deep Learning, Survey, ToRead},
	file = {arXiv.org Snapshot:D\:\\download\\Zotero\\storage\\YC4DBKW2\\1709.html:text/html;Hamilton et al_2017_Representation Learning on Graphs.pdf:D\:\\download\\Zotero\\storage\\KGYVX5KK\\Hamilton et al_2017_Representation Learning on Graphs.pdf:application/pdf}
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
	booktitle = {{NIPS}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
	file = {Vaswani et al. - Attention is All you Need.pdf:D\:\\download\\Zotero\\storage\\ZABBIZIF\\Vaswani et al. - Attention is All you Need.pdf:application/pdf}
}

@article{goodfellow_generative_nodate,
	title = {Generative {Adversarial} {Nets}},
	language = {en},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	pages = {9},
	file = {Goodfellow et al. - Generative Adversarial Nets.pdf:D\:\\download\\Zotero\\storage\\QP9UZL23\\Goodfellow et al. - Generative Adversarial Nets.pdf:application/pdf}
}

@article{cai_comprehensive_2018,
	title = {A {Comprehensive} {Survey} of {Graph} {Embedding}: {Problems}, {Techniques} and {Applications}},
	issn = {1041-4347},
	shorttitle = {A {Comprehensive} {Survey} of {Graph} {Embedding}},
	doi = {10/gdcfq2},
	abstract = {Graph is an important data representation which appears in a wide diversity of real-world scenarios. Effective graph analytics provides users a deeper understanding of what is behind the data, and thus can benefit a lot of useful applications such as node classification, node recommendation, link prediction, etc. However, most graph analytics methods suffer the high computation and space cost. Graph embedding is an effective yet efficient way to solve the graph analytics problem. It converts the graph data into a low dimensional space in which the graph structural information and graph properties are maximumly preserved. In this survey, we conduct a comprehensive review of the literature in graph embedding. We first introduce the formal definition of graph embedding as well as the related concepts. After that, we propose two taxonomies of graph embedding which correspond to what challenges exist in different graph embedding problem settings and how the existing work address these challenges in their solutions. Finally, we summarize the applications that graph embedding enables and suggest four promising future research directions in terms of computation efficiency, problem settings, techniques and application scenarios.},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Cai, H. and Zheng, V. W. and Chang, K.},
	year = {2018},
	keywords = {Graph Deep Learning, Survey},
	pages = {1--1},
	file = {Cai et al_2018_A Comprehensive Survey of Graph Embedding.pdf:D\:\\download\\Zotero\\storage\\L8YSGFZH\\Cai et al. - 2018 - A Comprehensive Survey of Graph Embedding Problem.pdf:application/pdf;IEEE Xplore Abstract Record:D\:\\download\\Zotero\\storage\\YEUTMD4W\\8294302.html:text/html}
}

@article{goodfellow_nips_2016,
	title = {{NIPS} 2016 {Tutorial}: {Generative} {Adversarial} {Networks}},
	shorttitle = {{NIPS} 2016 {Tutorial}},
	url = {http://arxiv.org/abs/1701.00160},
	abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
	urldate = {2018-04-27},
	journal = {arXiv:1701.00160 [cs]},
	author = {Goodfellow, Ian},
	month = dec,
	year = {2016},
	file = {arXiv.org Snapshot:D\:\\download\\Zotero\\storage\\3LGCJTUH\\1701.html:text/html;Goodfellow_2016_NIPS 2016 Tutorial.pdf:D\:\\download\\Zotero\\storage\\RSYAYWKS\\Goodfellow_2016_NIPS 2016 Tutorial.pdf:application/pdf}
}

@article{belkin_manifold_2006,
	title = {Manifold regularization: {A} geometric framework for learning from labeled and unlabeled examples},
	volume = {7},
	issn = {1533-7928},
	shorttitle = {Manifold regularization},
	journal = {Journal of Machine Learning Research},
	author = {Belkin, M. and Niyogi, P. and Sindhwani, V.},
	year = {2006},
	keywords = {Graph Transduction, Kernel Methods, Manifold Learning, Regularization, Semi-Supervised Learning, Spectral Graph Theory, Support Vector Machines, Unlabeled Data},
	pages = {2399--2434}
}

@article{weston_deep_2012,
	title = {Deep learning via semi-supervised embedding},
	volume = {7700},
	issn = {0302-9743},
	doi = {10.1007/978-3-642-35289-8-34},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Weston, J. and Ratle, F. and Mobahi, H. and Collobert, R.},
	year = {2012},
	keywords = {\_Invalid DOI},
	pages = {639--655}
}

@inproceedings{getoor_link-based_2005,
	series = {Advanced {Information} and {Knowledge} {Processing}},
	title = {Link-based {Classification}},
	isbn = {978-1-85233-989-0 978-1-84628-284-3},
	url = {https://link.springer.com/chapter/10.1007/1-84628-284-5_7},
	doi = {10.1007/1-84628-284-5_7},
	abstract = {SummaryA key challenge for machine learning is the problem of mining richly structured data sets, where the objects are linked in some way due to either an explicit or implicit relationship that exists between the objects. Links among the objects demonstrate certain patterns, which can be helpful for many machine learning tasks and are usually hard to capture with traditional statistical models. Recently there has been a surge of interest in this area, fuelled largely by interest in web and hypertext mining, but also by interest in mining social networks, bibliographic citation data, epidemiological data and other domains best described using a linked or graph structure. In this chapter we propose a framework for modeling link distributions, a link-based model that supports discriminative models describing both the link distributions and the attributes of linked objects. We use a structured logistic regression model, capturing both content and links. We systematically evaluate several variants of our link-based model on a range of data sets including both web and citation collections. In all cases, the use of the link distribution improves classification performance.},
	language = {en},
	urldate = {2018-04-30},
	booktitle = {Advanced {Methods} for {Knowledge} {Discovery} from {Complex} {Data}},
	publisher = {Springer, London},
	author = {Getoor, Lise},
	year = {2005},
	pages = {189--207},
	file = {Snapshot:D\:\\download\\Zotero\\storage\\78F96TRC\\10.html:text/html}
}

@article{yang_revisiting_2016,
	title = {Revisiting {Semi}-{Supervised} {Learning} with {Graph} {Embeddings}},
	url = {http://arxiv.org/abs/1603.08861},
	abstract = {We present a semi-supervised learning framework based on graph embeddings. Given a graph between instances, we train an embedding for each instance to jointly predict the class label and the neighborhood context in the graph. We develop both transductive and inductive variants of our method. In the transductive variant of our method, the class labels are determined by both the learned embeddings and input feature vectors, while in the inductive variant, the embeddings are defined as a parametric function of the feature vectors, so predictions can be made on instances not seen during training. On a large and diverse set of benchmark tasks, including text classification, distantly supervised entity extraction, and entity classification, we show improved performance over many of the existing models.},
	urldate = {2018-04-30},
	journal = {arXiv:1603.08861 [cs]},
	author = {Yang, Zhilin and Cohen, William W. and Salakhutdinov, Ruslan},
	month = mar,
	year = {2016},
	keywords = {Computer Science - Learning},
	file = {arXiv.org Snapshot:D\:\\download\\Zotero\\storage\\WAKLUKYN\\1603.html:text/html;Yang et al_2016_Revisiting Semi-Supervised Learning with Graph Embeddings.pdf:D\:\\download\\Zotero\\storage\\M39XG3WQ\\Yang et al_2016_Revisiting Semi-Supervised Learning with Graph Embeddings.pdf:application/pdf}
}

@article{kipf_semi-supervised_2016,
	title = {Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1609.02907},
	abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
	urldate = {2018-04-30},
	journal = {arXiv:1609.02907 [cs, stat]},
	author = {Kipf, Thomas N. and Welling, Max},
	month = sep,
	year = {2016},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:D\:\\download\\Zotero\\storage\\TN7XYHFK\\1609.html:text/html;Kipf_Welling_2016_Semi-Supervised Classification with Graph Convolutional Networks.pdf:D\:\\download\\Zotero\\storage\\E6R97QHY\\Kipf_Welling_2016_Semi-Supervised Classification with Graph Convolutional Networks.pdf:application/pdf}
}

@inproceedings{monti_geometric_2017,
	title = {Geometric {Deep} {Learning} on {Graphs} and {Manifolds} {Using} {Mixture} {Model} {CNNs}},
	doi = {10.1109/CVPR.2017.576},
	abstract = {Deep learning has achieved a remarkable performance breakthrough in several fields, most notably in speech recognition, natural language processing, and computer vision. In particular, convolutional neural network (CNN) architectures currently produce state-of-the-art performance on a variety of image analysis tasks such as object detection and recognition. Most of deep learning research has so far focused on dealing with 1D, 2D, or 3D Euclidean-structured data such as acoustic signals, images, or videos. Recently, there has been an increasing interest in geometric deep learning, attempting to generalize deep learning methods to non-Euclidean structured data such as graphs and manifolds, with a variety of applications from the domains of network analysis, computational social science, or computer graphics. In this paper, we propose a unified framework allowing to generalize CNN architectures to non-Euclidean domains (graphs and manifolds) and learn local, stationary, and compositional task-specific features. We show that various non-Euclidean CNN methods previously proposed in the literature can be considered as particular instances of our framework. We test the proposed method on standard tasks from the realms of image-, graph-and 3D shape analysis and show that it consistently outperforms previous approaches.},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Monti, F. and Boscaini, D. and Masci, J. and Rodolà, E. and Svoboda, J. and Bronstein, M. M.},
	month = jul,
	year = {2017},
	keywords = {3D shape analysis, CNN, Computational modeling, computer vision, Convolution, convolutional neural network, geometric deep learning, graph analysis, graphs, image analysis, Laplace equations, learning (artificial intelligence), Machine learning, manifolds, Manifolds, mixture model, network analysis, neural nets, nonEuclidean CNN methods, nonEuclidean structured data, object detection, Shape, Three-dimensional displays},
	pages = {5425--5434},
	file = {IEEE Xplore Abstract Record:D\:\\download\\Zotero\\storage\\6KFM6AX4\\8100059.html:text/html;Monti et al_2017_Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs.pdf:D\:\\download\\Zotero\\storage\\BNYQRYUN\\Monti et al_2017_Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs.pdf:application/pdf}
}

@inproceedings{zhu_semi-supervised_2003,
	title = {Semi-{Supervised} {Learning} {Using} {Gaussian} {Fields} and {Harmonic} {Functions}},
	abstract = {An approach to semi-supervised learning is proposed that is based on a Gaussian random ﬁeld model. Labeled and unlabeled data are represented as vertices in a weighted graph, with edge weights encoding the similarity between instances. The learning problem is then formulated in terms of a Gaussian random ﬁeld on this graph, where the mean of the ﬁeld is characterized in terms of harmonic functions, and is efﬁciently obtained using matrix methods or belief propagation. The resulting learning algorithms have intimate connections with random walks, electric networks, and spectral graph theory. We discuss methods to incorporate class priors and the predictions of classiﬁers obtained by supervised learning. We also propose a method of parameter learning by entropy minimization, and show the algorithm’s ability to perform feature selection. Promising experimental results are presented for synthetic data, digit classiﬁcation, and text classiﬁcation tasks.},
	language = {en},
	booktitle = {{ICML}},
	author = {Zhu, Xiaojin and Ghahramani, Zoubin and Lafferty, John},
	year = {2003},
	pages = {8},
	file = {Zhu et al. - Semi-Supervised Learning Using Gaussian Fields and.pdf:D\:\\download\\Zotero\\storage\\HH436UUW\\Zhu et al. - Semi-Supervised Learning Using Gaussian Fields and.pdf:application/pdf}
}

@inproceedings{perozzi_deepwalk:_2014,
	address = {New York, NY, USA},
	series = {{KDD} '14},
	title = {{DeepWalk}: {Online} {Learning} of {Social} {Representations}},
	isbn = {978-1-4503-2956-9},
	shorttitle = {{DeepWalk}},
	url = {http://doi.acm.org/10.1145/2623330.2623732},
	doi = {10.1145/2623330.2623732},
	abstract = {We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide F1 scores up to 10\% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60\% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.},
	urldate = {2018-04-30},
	booktitle = {Proceedings of the 20th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
	year = {2014},
	keywords = {deep learning, latent representations, learning with partial labels, network classification, online learning, social networks},
	pages = {701--710},
	file = {Perozzi et al_2014_DeepWalk.pdf:D\:\\download\\Zotero\\storage\\MNBF4Y8C\\Perozzi et al_2014_DeepWalk.pdf:application/pdf}
}

@article{li_gated_2015,
	title = {Gated {Graph} {Sequence} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.05493},
	abstract = {Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.},
	urldate = {2018-04-30},
	journal = {arXiv:1511.05493 [cs, stat]},
	author = {Li, Yujia and Tarlow, Daniel and Brockschmidt, Marc and Zemel, Richard},
	month = nov,
	year = {2015},
	note = {00148 
arXiv: 1511.05493},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:D\:\\download\\Zotero\\storage\\VZSRINF7\\1511.html:text/html;Li et al_2015_Gated Graph Sequence Neural Networks.pdf:D\:\\download\\Zotero\\storage\\FQWMHNXE\\Li et al_2015_Gated Graph Sequence Neural Networks.pdf:application/pdf}
}

@article{bahdanau_neural_2014,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	urldate = {2018-05-01},
	journal = {arXiv:1409.0473 [cs, stat]},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = sep,
	year = {2014},
	note = {03206 
arXiv: 1409.0473},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:D\:\\download\\Zotero\\storage\\V7CMXCVF\\1409.html:text/html;Bahdanau et al_2014_Neural Machine Translation by Jointly Learning to Align and Translate.pdf:D\:\\download\\Zotero\\storage\\WFRFUGNL\\Bahdanau et al_2014_Neural Machine Translation by Jointly Learning to Align and Translate.pdf:application/pdf}
}

@article{bruna_spectral_2013,
	title = {Spectral {Networks} and {Locally} {Connected} {Networks} on {Graphs}},
	url = {http://arxiv.org/abs/1312.6203},
	abstract = {Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.},
	urldate = {2018-05-01},
	journal = {arXiv:1312.6203 [cs]},
	author = {Bruna, Joan and Zaremba, Wojciech and Szlam, Arthur and LeCun, Yann},
	month = dec,
	year = {2013},
	note = {00235 
arXiv: 1312.6203},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:D\:\\download\\Zotero\\storage\\YVEJFII4\\1312.html:text/html;Bruna et al_2013_Spectral Networks and Locally Connected Networks on Graphs.pdf:D\:\\download\\Zotero\\storage\\KR9TXQC6\\Bruna et al_2013_Spectral Networks and Locally Connected Networks on Graphs.pdf:application/pdf}
}